{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Beyond Spell-Checking: Word-Checking</h1>\n",
    "<h2>An attention based Transformer approach</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Abdurrahman Shahid</h2>\n",
    "<h3>L3 MIASHS - SC, University of Lille, France</h3>\n",
    "<h3>Study and Research Work - Travaux d’Etude et de Recherche (TER) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatfile(inp_dir, out_dir, year_from=2007, year_to=2019, lower=True):\n",
    "    \n",
    "    years = [year for year in range(year_from,year_to+1)]\n",
    "    print(\"Years considered: {}\\n\".format(years))\n",
    "    \n",
    "    filename_out = \"NewsConcat_{}-{}\".format(year_from,year_to)\n",
    "    if lower:\n",
    "        filename_out = filename_out + \"_lowercase\"\n",
    "    filename_out = filename_out + \".txt\"\n",
    "    outputfile = open(os.path.join(out_dir,filename_out), \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    lines_count = {}\n",
    "    for year in years:\n",
    "        filename = \"news.\"+str(year)+\".fr.shuffled.txt\"\n",
    "        filepath = os.path.join(inp_dir, filename)\n",
    "        file = open(filepath, \"r\", encoding=\"utf-8\" )\n",
    "        print(\"start processing {}\".format(filename))\n",
    "        stop = False\n",
    "        count_lines = 0\n",
    "        while not stop:\n",
    "            line = file.readline()\n",
    "            if lower:\n",
    "                line = line.lower()\n",
    "            count_lines += 1\n",
    "            outputfile.write(line)\n",
    "            if not line:\n",
    "                stop = True\n",
    "                print(\"finished processing {}\".format(filename))\n",
    "                file.close()\n",
    "                lines_count[filename] = count_lines\n",
    "    outputfile.close()\n",
    "    end_time = datetime.now()\n",
    "    delta = end_time - start_time\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"\\nFile created with success (~{} {}): \\n{}\\n\".format(d, d_unit, filename_out))\n",
    "    \n",
    "    for k in list(lines_count.keys()):\n",
    "        print(\"{}: {} lines\".format(k,lines_count[k]))\n",
    "    print(\"Total lines in new file: {}\".format(sum(list(lines_count.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years considered: [2007, 2008, 2009, 2010, 2011, 2012]\n",
      "\n",
      "start processing news.2007.fr.shuffled.txt\n",
      "finished processing news.2007.fr.shuffled.txt\n",
      "start processing news.2008.fr.shuffled.txt\n",
      "finished processing news.2008.fr.shuffled.txt\n",
      "start processing news.2009.fr.shuffled.txt\n",
      "finished processing news.2009.fr.shuffled.txt\n",
      "start processing news.2010.fr.shuffled.txt\n",
      "finished processing news.2010.fr.shuffled.txt\n",
      "start processing news.2011.fr.shuffled.txt\n",
      "finished processing news.2011.fr.shuffled.txt\n",
      "start processing news.2012.fr.shuffled.txt\n",
      "finished processing news.2012.fr.shuffled.txt\n",
      "\n",
      "File created with success (~1 minutes): \n",
      "NewsConcat_2007-2012_lowercase.txt\n",
      "\n",
      "news.2007.fr.shuffled.txt: 118959 lines\n",
      "news.2008.fr.shuffled.txt: 4718842 lines\n",
      "news.2009.fr.shuffled.txt: 4366673 lines\n",
      "news.2010.fr.shuffled.txt: 1846494 lines\n",
      "news.2011.fr.shuffled.txt: 6030153 lines\n",
      "news.2012.fr.shuffled.txt: 4337029 lines\n",
      "Total lines in new file: 21418150\n"
     ]
    }
   ],
   "source": [
    "concatfile(\"Raw_Data\",\"Clean_Data\", year_from=2007, year_to=2012, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones = [\n",
    "    (\"a\",\"à\"),\n",
    "    (\"est\",\"et\"),\n",
    "    (\"ces\",\"ses\"),\n",
    "    (\"ce\",\"se\"),\n",
    "    (\"ou\",\"où\"),\n",
    "    (\"la\",\"là\"),\n",
    "    (\"tout\",\"tous\"),\n",
    "    (\"leur\",\"leurs\"),\n",
    "    (\"ceux\",\"ce\"),\n",
    "    (\"cette\",\"cet\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_sentences_length(homophones, inp_dir, out_dir, filename_inp, lmin, lmax):\n",
    "    \"\"\"Preprocess to keep sentences with length within lmin and lmax and containing at least one of the homophone\n",
    "       present in input=homophones\"\"\"\n",
    "    \n",
    "    h_liste = []\n",
    "    for a, b in homophones:\n",
    "        if a not in h_liste:\n",
    "            h_liste.append(a)\n",
    "        if b not in h_liste:\n",
    "            h_liste.append(b)\n",
    "     \n",
    "    file = open(os.path.join(inp_dir, filename_inp), \"r\", encoding=\"utf-8\")\n",
    "    filename_out = filename_inp[:-4] + \"_limit_{}-{}w.txt\".format(lmin, lmax)\n",
    "    file_out = open(os.path.join(out_dir, filename_out), \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    lines_nb = 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        line = file.readline()\n",
    "        to_w = False\n",
    "        len_l = len(line.split(' '))\n",
    "        if len_l>= lmin and len_l <= lmax:\n",
    "            for w in h_liste:\n",
    "                target = \" \"+ w +\" \"\n",
    "                if target in line:\n",
    "                    to_w = True\n",
    "            if to_w:\n",
    "                file_out.write(line)\n",
    "                lines_nb += 1\n",
    "        \n",
    "        if not line:\n",
    "            stop = True\n",
    "            file.close()\n",
    "            file_out.close()\n",
    "    \n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"\\nFile created with success ({} lines in new file, ~{} {}): \\n{}\\n\".format(\n",
    "                                                                                    lines_nb, d, d_unit, filename_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File created with success (7495691 lines in new file, ~1 minutes): \n",
      "NewsConcat_2007-2012_lowercase_limit_5-20w.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit_sentences_length(homophones,\"Clean_Data\",\"Clean_Data\",\"NewsConcat_2007-2012_lowercase.txt\",5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(homophones, inp_dir, filename_data, save=False, savename=False):\n",
    "    \"\"\"Returns all line numbers for each homophone that appears in filename_data.\n",
    "    output: dictionary containing for each homophone a list of line numbers where this homophone occurs.\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    dataset = open(os.path.join(inp_dir, filename_data), 'r', encoding=\"utf-8\")\n",
    "    \n",
    "    out_dic = {}\n",
    "    h_liste = []\n",
    "    for a, b in homophones:\n",
    "        if a not in h_liste:\n",
    "            h_liste.append(a)\n",
    "        if b not in h_liste:\n",
    "            h_liste.append(b)\n",
    "    \n",
    "    for h in h_liste:\n",
    "        out_dic[h] = []\n",
    "        \n",
    "    stop = False\n",
    "    line_num = 0\n",
    "    while not stop:\n",
    "        line = dataset.readline()\n",
    "        line_num +=1\n",
    "        for h in h_liste:\n",
    "            target = \" \" + h + \" \"\n",
    "            if target in line:\n",
    "                out_dic[h].append(line_num)\n",
    "        if not line:\n",
    "            stop = True\n",
    "            \n",
    "    dataset.close()\n",
    "    \n",
    "    if save:\n",
    "        if savename:\n",
    "            outputname = savename + \".pickle\"\n",
    "            pickle.dump(out_dic, open(outputname, \"wb\"))\n",
    "        if not savename:\n",
    "            outputname = \"homophones_location-\"+filename_data[:-4] + \".pickle\"\n",
    "            pickle.dump(out_dic, open(outputname, \"wb\"))\n",
    "        end = datetime.now()\n",
    "        delta = end - start\n",
    "        delta_minutes = delta.seconds // 60\n",
    "        d = delta_minutes\n",
    "        d_unit = \"minutes\"\n",
    "        if delta_minutes == 0:\n",
    "            d = delta.seconds\n",
    "            d_unit = \"seconds\"\n",
    "        print(\"\\nDictionary created with success (~{} {}): \\n{}\\n\".format(d, d_unit, outputname))\n",
    "    if not save:\n",
    "        end = datetime.now()\n",
    "        delta = end - start\n",
    "        delta_minutes = delta.seconds // 60\n",
    "        d = delta_minutes\n",
    "        d_unit = \"minutes\"\n",
    "        if delta_minutes == 0:\n",
    "            d = delta.seconds\n",
    "            d_unit = \"seconds\"\n",
    "        print(\"\\nget_location function executed successfully (~{} {})\\n\".format(d, d_unit))\n",
    "    return out_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_location function executed successfully (~39 seconds)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_dict = get_location(homophones, \"Clean_Data\", \"NewsConcat_2007-2012_lowercase_limit_5-20w.txt\", save=False, savename=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_number(sets, printmes=True, ret=False):\n",
    "    h_rank = []\n",
    "    for key in list(sets.keys()):\n",
    "        num_sentence = len(sets[key])\n",
    "        h_rank.append((num_sentence, key))\n",
    "        if printmes:\n",
    "            print(\"homophone: {}, number of sentences: {} (~{}k)\".format(key,num_sentence, round(num_sentence/10**3)))\n",
    "    \n",
    "    if ret:\n",
    "        h_rank.sort()\n",
    "        return h_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homophone: a, number of sentences: 1463794 (~1464k)\n",
      "homophone: à, number of sentences: 2471098 (~2471k)\n",
      "homophone: est, number of sentences: 900333 (~900k)\n",
      "homophone: et, number of sentences: 1790874 (~1791k)\n",
      "homophone: ces, number of sentences: 157747 (~158k)\n",
      "homophone: ses, number of sentences: 257173 (~257k)\n",
      "homophone: ce, number of sentences: 493528 (~494k)\n",
      "homophone: se, number of sentences: 568760 (~569k)\n",
      "homophone: ou, number of sentences: 195584 (~196k)\n",
      "homophone: où, number of sentences: 79319 (~79k)\n",
      "homophone: la, number of sentences: 2757402 (~2757k)\n",
      "homophone: là, number of sentences: 33093 (~33k)\n",
      "homophone: tout, number of sentences: 204412 (~204k)\n",
      "homophone: tous, number of sentences: 101203 (~101k)\n",
      "homophone: leur, number of sentences: 208740 (~209k)\n",
      "homophone: leurs, number of sentences: 97720 (~98k)\n",
      "homophone: ceux, number of sentences: 27904 (~28k)\n",
      "homophone: cette, number of sentences: 292243 (~292k)\n",
      "homophone: cet, number of sentences: 55014 (~55k)\n"
     ]
    }
   ],
   "source": [
    "h_rank = sentence_number(location_dict, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_for_tokenizer(homophones, inp_dir, inp_file, num_h = 25000):\n",
    "    file = open(os.path.join(inp_dir, inp_file), \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    h_dic = {}\n",
    "    for a, b in homophones:\n",
    "        if a not in list(h_dic.keys()):\n",
    "            h_dic[a] = 0\n",
    "        if b not in list(h_dic.keys()):\n",
    "            h_dic[b] = 0\n",
    "    \n",
    "    filename_out = \"file_train_tokenizer-{}lines.txt\".format(int(len(h_dic.keys())*num_h))\n",
    "    file_out = open(filename_out, \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    counter=0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        line = file.readline()\n",
    "        for w in list(h_dic.keys()):\n",
    "            target = \" \"+ w +\" \"\n",
    "            if target in line and h_dic[w] < num_h:\n",
    "                file_out.write(line)\n",
    "                h_dic[w] += 1\n",
    "                counter += 1\n",
    "                break\n",
    "                \n",
    "        if len(h_dic.keys())*num_h == counter or (not line):\n",
    "            stop = True\n",
    "            file.close()\n",
    "            file_out.close()\n",
    "    \n",
    "    \n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"\\nFile created with success ({} lines in new file, ~{} {}): \\n{}\\n\".format(counter, d, d_unit, filename_out))\n",
    "    print(\"{}\".format(h_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File created with success (380000 lines in new file, ~33 seconds): \n",
      "file_train_tokenizer-380000lines.txt\n",
      "\n",
      "{'a': 20000, 'à': 20000, 'est': 20000, 'et': 20000, 'ces': 20000, 'ses': 20000, 'ce': 20000, 'se': 20000, 'ou': 20000, 'où': 20000, 'la': 20000, 'là': 20000, 'tout': 20000, 'tous': 20000, 'leur': 20000, 'leurs': 20000, 'ceux': 20000, 'cette': 20000, 'cet': 20000}\n"
     ]
    }
   ],
   "source": [
    "create_file_for_tokenizer(homophones,\"Clean_Data\",\"NewsConcat_2007-2012_lowercase_limit_5-20w.txt\",20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer(filename_data, vocab_size, save=False):\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    filename_data = os.path.join(filename_data)\n",
    "    dataset = tf.data.TextLineDataset(filename_data)\n",
    "    tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "        (line.numpy().decode(\"utf-8\") for line in dataset), vocab_size)\n",
    "    \n",
    "    if save:\n",
    "        backup_name = \"tokenizer_{}_{}\".format(tokenizer.vocab_size,filename_data)\n",
    "        tokenizer.save_to_file(backup_name)\n",
    "    \n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    if save:\n",
    "        print(\"\\nTokenizer created with success (~{} {}):\\n{}\\n\".format(d, d_unit, backup_name))\n",
    "    if not save:\n",
    "        print(\"\\nTokenizer created with success (~{} {}).\".format(d, d_unit))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer created with success (~1 minutes):\n",
      "tokenizer_1034_file_train_tokenizer-380000lines.txt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=1034>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_tokenizer(\"file_train_tokenizer-380000lines.txt\", 1024, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1024 = tfds.features.text.SubwordTextEncoder.load_from_file(\n",
    "                                                            \"tokenizer_1034_file_train_tokenizer-380000lines.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clean_data(inp_dir, inp_file, tokenizer, limit):\n",
    "    dirtydata = open(os.path.join(inp_dir, inp_file), \"r\", encoding=\"utf-8\")\n",
    "    output_name = \"Selected_data\" + str(\"-vs\") + str(tokenizer.vocab_size) + str(\"-toklim\") + str(limit) + \".txt\"\n",
    "    cleandata = open(os.path.join(inp_dir, output_name), \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    lines_nb = 0\n",
    "    write_nb = 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        line = dirtydata.readline()\n",
    "        lines_nb += 1\n",
    "        if lines_nb % 1000000 == 0:\n",
    "            print(\"lines treated until now: {} ({} Million)\".format(lines_nb, round(lines_nb//1000000)))\n",
    "        len_line = len(tokenizer.encode(line))\n",
    "        if len_line <= limit:\n",
    "            cleandata.write(line)\n",
    "            write_nb += 1\n",
    "        if not line:\n",
    "            stop = True\n",
    "    \n",
    "    dirtydata.close()\n",
    "    cleandata.close()\n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"\\nFile created with success ({} lines in new file, ~{} {}): \\n{}\\n\".format(\n",
    "                                                                                    write_nb, d, d_unit, output_name))\n",
    "    print(\"Total number of lines treated: {} (~{} M)\".format(lines_nb, lines_nb//1000000))\n",
    "    print(\"Total number of lines kept: {} (~{} M)\".format(write_nb, write_nb//1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines treated until now: 1000000 (1 Million)\n",
      "lines treated until now: 2000000 (2 Million)\n",
      "lines treated until now: 3000000 (3 Million)\n",
      "lines treated until now: 4000000 (4 Million)\n",
      "lines treated until now: 5000000 (5 Million)\n",
      "lines treated until now: 6000000 (6 Million)\n",
      "lines treated until now: 7000000 (7 Million)\n",
      "\n",
      "File created with success (2787248 lines in new file, ~4 minutes): \n",
      "Selected_data-vs1034-toklim30.txt\n",
      "\n",
      "Total number of lines treated: 7495692 (~7 M)\n",
      "Total number of lines kept: 2787248 (~2 M)\n"
     ]
    }
   ],
   "source": [
    "select_clean_data(\"Clean_Data\",\"NewsConcat_2007-2012_lowercase_limit_5-20w.txt\", tokenizer_1024, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_location function executed successfully (~12 seconds)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_dict = get_location(homophones, \"Clean_Data\", \"Selected_data-vs1034-toklim30.txt\", save=False, savename=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homophone: a, number of sentences: 455567 (~456k)\n",
      "homophone: à, number of sentences: 788817 (~789k)\n",
      "homophone: est, number of sentences: 368907 (~369k)\n",
      "homophone: et, number of sentences: 452475 (~452k)\n",
      "homophone: ces, number of sentences: 46899 (~47k)\n",
      "homophone: ses, number of sentences: 76241 (~76k)\n",
      "homophone: ce, number of sentences: 175835 (~176k)\n",
      "homophone: se, number of sentences: 206140 (~206k)\n",
      "homophone: ou, number of sentences: 49183 (~49k)\n",
      "homophone: où, number of sentences: 18021 (~18k)\n",
      "homophone: la, number of sentences: 875422 (~875k)\n",
      "homophone: là, number of sentences: 15906 (~16k)\n",
      "homophone: tout, number of sentences: 87980 (~88k)\n",
      "homophone: tous, number of sentences: 38460 (~38k)\n",
      "homophone: leur, number of sentences: 63657 (~64k)\n",
      "homophone: leurs, number of sentences: 25830 (~26k)\n",
      "homophone: ceux, number of sentences: 7737 (~8k)\n",
      "homophone: cette, number of sentences: 93773 (~94k)\n",
      "homophone: cet, number of sentences: 17350 (~17k)\n"
     ]
    }
   ],
   "source": [
    "h_rank = sentence_number(location_dict, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File created with success (133000 lines in new file, ~12 seconds): \n",
      "file_train_tokenizer-133000lines.txt\n",
      "\n",
      "{'a': 7000, 'à': 7000, 'est': 7000, 'et': 7000, 'ces': 7000, 'ses': 7000, 'ce': 7000, 'se': 7000, 'ou': 7000, 'où': 7000, 'la': 7000, 'là': 7000, 'tout': 7000, 'tous': 7000, 'leur': 7000, 'leurs': 7000, 'ceux': 7000, 'cette': 7000, 'cet': 7000}\n"
     ]
    }
   ],
   "source": [
    "create_file_for_tokenizer(homophones,\"Clean_Data\",\"Selected_data-vs1034-toklim30.txt\", 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer created with success (~1 minutes):\n",
      "tokenizer_1024_file_train_tokenizer-133000lines.txt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=1024>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_tokenizer(\"file_train_tokenizer-133000lines.txt\", 1024, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1024b = tfds.features.text.SubwordTextEncoder.load_from_file(\n",
    "                                                            \"tokenizer_1024_file_train_tokenizer-133000lines.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines treated until now: 1000000 (1 Million)\n",
      "lines treated until now: 2000000 (2 Million)\n",
      "\n",
      "File created with success (2701227 lines in new file, ~1 minutes): \n",
      "Selected_data-vs1024-toklim30.txt\n",
      "\n",
      "Total number of lines treated: 2787248 (~2 M)\n",
      "Total number of lines kept: 2701227 (~2 M)\n"
     ]
    }
   ],
   "source": [
    "select_clean_data(\"Clean_Data\",\"Selected_data-vs1034-toklim30.txt\", tokenizer_1024b, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ggle_forms(inp_dir, inp_file, ggle_forms):\n",
    "    dirtydata = open(os.path.join(inp_dir, inp_file), \"r\", encoding=\"utf-8\")\n",
    "    output_name = inp_file\n",
    "    cleandata = open(output_name, \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    list_sentences = []\n",
    "    \n",
    "    forms = open(ggle_forms, \"r\", encoding=\"utf-8\")\n",
    "    counter_sentences = 0\n",
    "    stop_forms = False\n",
    "    while not stop_forms:\n",
    "        line = forms.readline()\n",
    "        if line:\n",
    "            list_sentences.append(line.strip())\n",
    "            counter_sentences += 1\n",
    "        else:\n",
    "            stop_forms = True\n",
    "    \n",
    "    forms.close()\n",
    "    print(\"Number of sentences to search: {},(len list = {})\".format(counter_sentences, len(list_sentences)))\n",
    "    \n",
    "    index = []\n",
    "    lines_nb = 0\n",
    "    write_nb = 0\n",
    "    counter = 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        line = dirtydata.readline()\n",
    "        lines_nb += 1\n",
    "        if line.strip() in list_sentences:\n",
    "            counter += 1\n",
    "            print(\"sentence matched ({}/{}): {}\".format(counter,counter_sentences,line.strip()))\n",
    "            index.append(list_sentences.index(line.strip()))\n",
    "        if line.strip() not in list_sentences:\n",
    "            cleandata.write(line)\n",
    "            write_nb += 1\n",
    "        if lines_nb % 1000000 == 0:\n",
    "            print(\"\\nlines treated until now: {} ({} Million)\".format(lines_nb, round(lines_nb//1000000)))\n",
    "        if not line:\n",
    "            stop = True\n",
    "    \n",
    "    dirtydata.close()\n",
    "    cleandata.close()\n",
    "    \n",
    "    index_set = set(index)\n",
    "    print(\"\\nTotal sentences matched: {}\".format(len(index_set)))\n",
    "    \n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"\\nFile created with success ({} lines in new file, ~{} {}): \\n{}\\n\".format(\n",
    "                                                                                    write_nb, d, d_unit, output_name))\n",
    "    print(\"Total number of lines treated: {} (~{} M)\".format(lines_nb, lines_nb//1000000))\n",
    "    print(\"Total number of lines kept: {} (~{} M)\".format(write_nb, write_nb//1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences to search: 40,(len list = 40)\n",
      "sentence matched (1/40): les studios de cinéma sont célèbres pour leur créativité comptable.\n",
      "sentence matched (2/40): ce niveau délimite la borne basse d'un mini canal horizontal.\n",
      "sentence matched (3/40): face à cette situation de surchauffe, une nette correction a été engagée.\n",
      "sentence matched (4/40): mais quand t'es embarqué dans l'engrenage, tu ne penses même pas à ça.\n",
      "sentence matched (5/40): cela fuse dans tous les sens, c’est du grand n’importe quoi.\n",
      "sentence matched (6/40): que du bonheur pour ceux qui sauront dépasser leurs douleurs.\n",
      "sentence matched (7/40): la pratique courante de l'anglais est un atout.\n",
      "sentence matched (8/40): les studios de cinéma sont célèbres pour leur créativité comptable.\n",
      "sentence matched (9/40): les inscriptions se feront pendant les heures d'entrainements.\n",
      "sentence matched (10/40): la vente de carte journalière au tarif de 8 € se fera sur place.\n",
      "sentence matched (11/40): puisqu'en mode comme en fleurissement, les tendances se font et se défont.\n",
      "sentence matched (12/40): une chose est sûre, ce n'était pas de la fausse monnaie !\n",
      "sentence matched (13/40): je suis là pour marquer, dans une équipe qui pratique un jeu ouvert.\n",
      "sentence matched (14/40): il a fait valoir que les tarifs du câble ont été réglementés aux états-unis.\n",
      "sentence matched (15/40): le thème de cette dixième édition était « fêtes et traditions ».\n",
      "sentence matched (16/40): beaucoup d'universitaires poursuivent une partie de leurs cours.\n",
      "sentence matched (17/40): c'est évidemment peu, ou déjà trop.\n",
      "sentence matched (18/40): washington pourrait ainsi satisfaire ses partenaires internationaux.\n",
      "sentence matched (19/40): plus de 34 millions de passagers transitent par cet aéroport chaque année.\n",
      "sentence matched (20/40): jusqu'où peut-on protéger ces chers rejetons de notre chair ?\n",
      "\n",
      "lines treated until now: 1000000 (1 Million)\n",
      "sentence matched (21/40): son atelier, il est à son image : cool.\n",
      "sentence matched (22/40): et ceux d'aujourd'hui, sont-ils déjà au point ?\n",
      "sentence matched (23/40): son chat, lui, ronronne à nouveau, auprès d’elle.\n",
      "sentence matched (24/40): je vais poursuivre ce genre d'activités.\n",
      "sentence matched (25/40): les particuliers peuvent y déposer leurs cartouches usagées.\n",
      "sentence matched (26/40): un bon moment de détente où le rire sera le fil conducteur de cette soirée.\n",
      "sentence matched (27/40): tu es là, maintenant, et tu n'es plus en 1994 ou en 1996.\n",
      "sentence matched (28/40): le journal a appris de nouveaux détails sur cette affaire.\n",
      "sentence matched (29/40): en plus, il y a ce texte-là où je leur impose une vision.\n",
      "sentence matched (30/40): comment faire pour être comme tout le monde?\n",
      "sentence matched (31/40): si la réponse à ça, c'est oui, je l'entends.\n",
      "sentence matched (32/40): les cadeaux de noël ne sont pas toujours là où on le croit.\n",
      "sentence matched (33/40): c'est probablement la mesure la plus proche du ressenti de ces derniers.\n",
      "sentence matched (34/40): je ne comprends pas ce débalancement entre régions.\n",
      "sentence matched (35/40): sauront-ils exploiter la technologie de cet engin furtif ?\n",
      "sentence matched (36/40): au début, ce n’était pas évident, donc j’ai fait très attention.\n",
      "sentence matched (37/40): ça m’intéresse comme tous les gens de ma génération, mais sans plus.\n",
      "sentence matched (38/40): il a bon espoir de voir la construction aller de l'avant d'ici un an.\n",
      "sentence matched (39/40): reportons-nous au tout début des années 50.\n",
      "\n",
      "lines treated until now: 2000000 (2 Million)\n",
      "sentence matched (40/40): il faut maintenant que la france se donne les moyens de ses ambitions.\n",
      "\n",
      "Total sentences matched: 39\n",
      "\n",
      "File created with success (2701187 lines in new file, ~6 seconds): \n",
      "Selected_data-vs1024-toklim30.txt\n",
      "\n",
      "Total number of lines treated: 2701227 (~2 M)\n",
      "Total number of lines kept: 2701187 (~2 M)\n"
     ]
    }
   ],
   "source": [
    "filter_ggle_forms(\"Clean_Data\",\"Selected_data-vs1024-toklim30.txt\",\"test_sentence_ggle_forms_correct.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(fname, kfname):\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    list_sentences = []\n",
    "    \n",
    "    kf = open(kfname, \"r\", encoding=\"utf-8\")\n",
    "    counter_sentences = 0\n",
    "    stop_kf = False\n",
    "    while not stop_kf:\n",
    "        line = kf.readline()\n",
    "        if line:\n",
    "            list_sentences.append(line.strip())\n",
    "            counter_sentences += 1\n",
    "        else:\n",
    "            stop_kf = True\n",
    "    \n",
    "    kf.close()\n",
    "    print(\"Number of sentences to search: {},(len list = {})\".format(counter_sentences, len(list_sentences)))\n",
    "    \n",
    "    index = []\n",
    "    counter = 0\n",
    "    f = open(fname, \"r\", encoding=\"utf-8\")\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        line = f.readline()\n",
    "        if line.strip() in list_sentences:\n",
    "            counter += 1\n",
    "            print(\"sentence matched ({}/{}): {}\".format(counter,counter_sentences,line.strip()))\n",
    "            index.append(list_sentences.index(line.strip()))\n",
    "        if not line:\n",
    "            stop = True\n",
    "    f.close()\n",
    "    \n",
    "    index_set = set(index)\n",
    "    print(\"Total sentences matched: {}\\n\".format(len(index_set)))\n",
    "    \n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    \n",
    "    for s in list_sentences:\n",
    "        if list_sentences.index(s) not in index:\n",
    "            print(\"NOT FOUND ! {}, {}\".format(list_sentences.index(s),s))\n",
    "        else:\n",
    "            print(\"{}, {}\".format(list_sentences.index(s),s))\n",
    "    \n",
    "    print(\"\\nSearch done, (~{} {})\".format(d, d_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences to search: 40,(len list = 40)\n",
      "Total sentences matched: 0\n",
      "\n",
      "NOT FOUND ! 0, la vente de carte journalière au tarif de 8 € se fera sur place.\n",
      "NOT FOUND ! 1, sauront-ils exploiter la technologie de cet engin furtif ?\n",
      "NOT FOUND ! 2, c'est évidemment peu, ou déjà trop.\n",
      "NOT FOUND ! 3, face à cette situation de surchauffe, une nette correction a été engagée.\n",
      "NOT FOUND ! 4, puisqu'en mode comme en fleurissement, les tendances se font et se défont.\n",
      "NOT FOUND ! 5, le thème de cette dixième édition était « fêtes et traditions ».\n",
      "NOT FOUND ! 6, le journal a appris de nouveaux détails sur cette affaire.\n",
      "NOT FOUND ! 7, au début, ce n’était pas évident, donc j’ai fait très attention.\n",
      "NOT FOUND ! 8, si la réponse à ça, c'est oui, je l'entends.\n",
      "NOT FOUND ! 9, cela fuse dans tous les sens, c’est du grand n’importe quoi.\n",
      "NOT FOUND ! 10, tu es là, maintenant, et tu n'es plus en 1994 ou en 1996.\n",
      "NOT FOUND ! 11, c'est probablement la mesure la plus proche du ressenti de ces derniers.\n",
      "NOT FOUND ! 12, il a bon espoir de voir la construction aller de l'avant d'ici un an.\n",
      "NOT FOUND ! 13, washington pourrait ainsi satisfaire ses partenaires internationaux.\n",
      "NOT FOUND ! 14, que du bonheur pour ceux qui sauront dépasser leurs douleurs.\n",
      "NOT FOUND ! 15, beaucoup d'universitaires poursuivent une partie de leurs cours.\n",
      "NOT FOUND ! 16, son atelier, il est à son image : cool.\n",
      "NOT FOUND ! 17, plus de 34 millions de passagers transitent par cet aéroport chaque année.\n",
      "NOT FOUND ! 18, jusqu'où peut-on protéger ces chers rejetons de notre chair ?\n",
      "NOT FOUND ! 19, son chat, lui, ronronne à nouveau, auprès d’elle.\n",
      "NOT FOUND ! 20, comment faire pour être comme tout le monde?\n",
      "NOT FOUND ! 21, reportons-nous au tout début des années 50.\n",
      "NOT FOUND ! 22, je ne comprends pas ce débalancement entre régions.\n",
      "NOT FOUND ! 23, ce niveau délimite la borne basse d'un mini canal horizontal.\n",
      "NOT FOUND ! 24, les inscriptions se feront pendant les heures d'entrainements.\n",
      "NOT FOUND ! 25, il faut maintenant que la france se donne les moyens de ses ambitions.\n",
      "NOT FOUND ! 26, et ceux d'aujourd'hui, sont-ils déjà au point ?\n",
      "NOT FOUND ! 27, les cadeaux de noël ne sont pas toujours là où on le croit.\n",
      "NOT FOUND ! 28, je vais poursuivre ce genre d'activités.\n",
      "NOT FOUND ! 29, il a fait valoir que les tarifs du câble ont été réglementés aux états-unis.\n",
      "NOT FOUND ! 30, ça m’intéresse comme tous les gens de ma génération, mais sans plus.\n",
      "NOT FOUND ! 31, en plus, il y a ce texte-là où je leur impose une vision.\n",
      "NOT FOUND ! 32, un bon moment de détente où le rire sera le fil conducteur de cette soirée.\n",
      "NOT FOUND ! 33, la pratique courante de l'anglais est un atout.\n",
      "NOT FOUND ! 34, les européens veulent démontrer leur unité avant le g20 de londres.\n",
      "NOT FOUND ! 35, mais quand t'es embarqué dans l'engrenage, tu ne penses même pas à ça.\n",
      "NOT FOUND ! 36, une chose est sûre, ce n'était pas de la fausse monnaie !\n",
      "NOT FOUND ! 37, je suis là pour marquer, dans une équipe qui pratique un jeu ouvert.\n",
      "NOT FOUND ! 38, les particuliers peuvent y déposer leurs cartouches usagées.\n",
      "NOT FOUND ! 39, les studios de cinéma sont célèbres pour leur créativité comptable.\n",
      "\n",
      "Search done, (~2 seconds)\n"
     ]
    }
   ],
   "source": [
    "match(\"Selected_data-vs1024-toklim30.txt\",\"test_sentence_ggle_forms_correct.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Selected_data-vs1024-toklim30.txt in \\Clean_Data with the newly created file of the same name located in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_location function executed successfully (~11 seconds)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_dict = get_location(homophones, \"Clean_Data\", \"Selected_data-vs1024-toklim30.txt\", save=False, savename=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homophone: a, number of sentences: 439953 (~440k)\n",
      "homophone: à, number of sentences: 761414 (~761k)\n",
      "homophone: est, number of sentences: 358067 (~358k)\n",
      "homophone: et, number of sentences: 434448 (~434k)\n",
      "homophone: ces, number of sentences: 44913 (~45k)\n",
      "homophone: ses, number of sentences: 73573 (~74k)\n",
      "homophone: ce, number of sentences: 171115 (~171k)\n",
      "homophone: se, number of sentences: 199989 (~200k)\n",
      "homophone: ou, number of sentences: 46861 (~47k)\n",
      "homophone: où, number of sentences: 17374 (~17k)\n",
      "homophone: la, number of sentences: 844904 (~845k)\n",
      "homophone: là, number of sentences: 15606 (~16k)\n",
      "homophone: tout, number of sentences: 86107 (~86k)\n",
      "homophone: tous, number of sentences: 37370 (~37k)\n",
      "homophone: leur, number of sentences: 61553 (~62k)\n",
      "homophone: leurs, number of sentences: 24781 (~25k)\n",
      "homophone: ceux, number of sentences: 7470 (~7k)\n",
      "homophone: cette, number of sentences: 90617 (~91k)\n",
      "homophone: cet, number of sentences: 16806 (~17k)\n"
     ]
    }
   ],
   "source": [
    "h_rank = sentence_number(location_dict, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(homophones,h_rank,location_dict, trainsize = 20000, valsize=1/20, testsize=1/50):\n",
    "    \"\"\"\n",
    "    Returns a randomly sampled (without replacement) list of line numbers for each homophone in each couple of homophones.\n",
    "    Each line can appear only once during trainig, so once it has been used for a homophone it can not be used \n",
    "    for another one.\n",
    "    output: Dictionary containig for each homophone in each couple, a list of line numbers.\n",
    "    {h1:{h1_target:[...], h1_target_bis:[...]}, h2:{h2_target:[...]},...}\n",
    "    If a homophone h1 appears in two different couples, we use the other homophone of the couple as a key\n",
    "    in the dictionaries.\n",
    "    The set of all used line numbers is also returned.\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    \n",
    "    h_liste = [h_rank[i][1] for i in range(len(h_rank))]\n",
    "    num_to_select = int(trainsize*(1 + valsize + testsize))\n",
    "    out_dic = {h:{} for h in h_liste}\n",
    "    \n",
    "    used = set()\n",
    "    for h in h_liste:\n",
    "        #print(\"processing '{}'\".format(h))\n",
    "        for (a,b) in homophones:\n",
    "            if h==a or h==b:\n",
    "                target = b\n",
    "                if h==b:\n",
    "                    target = a\n",
    "                #print(\"match - '({},{})' with target = '{}'\".format(a,b,target))\n",
    "                h_set = set(location_dict[h])\n",
    "                h_set.difference_update(used)\n",
    "                h_set_list = list(h_set)\n",
    "                h_set_list_selected = list(np.random.choice(h_set_list, num_to_select, replace=False))\n",
    "                set_to_be_used = set(h_set_list_selected)\n",
    "                used.update(set_to_be_used)\n",
    "                out_dic[h][target] = h_set_list_selected\n",
    "                #print(\"length: {}\".format(len(h_set_list_selected)))\n",
    "                #print(\"Successfully created 'out_dic[{}][{}]'\".format(h,target))\n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"\\nget_split function executed successfully (~{} {})\\n\".format(d, d_unit))\n",
    "    \n",
    "    return out_dic, used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buil_train_val_test(homophones, location_dict, filename_data, trainsize = 20000, valsize=1/20, testsize=1/50):\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    h_rank = sentence_number(location_dict, printmes=False, ret=True)\n",
    "    # h_rank =[(num_s,key),...] , num_s = sentence number for homophone=key\n",
    "    \n",
    "    h_rank.sort()\n",
    "    \n",
    "    #sets_lists = pickle.load(open(\"NSelected_h_dicsets.pickle\", \"rb\"))\n",
    "    #sets_lists = get_set(homophones,filename_data, False)\n",
    "    #sets_lists = {h1:[line numbers], h2:[line numbers], ....}\n",
    "    \n",
    "    sets_lists_selected, used_set = get_split(homophones,h_rank,location_dict, \n",
    "                                                 trainsize = trainsize, valsize=valsize, testsize=testsize)\n",
    "    #sets_lists_selected = {h1:{h1_target:[], h1_target_bis:[]}, h2:{h2_target:[]},...}\n",
    "    now = datetime.now()\n",
    "    outfile_time= str(now.month)+\"-\"+str(now.day)+\"-\"+str(now.hour)+\"-\"+str(now.minute)\n",
    "    final_sets_name = \"final_sets_selected_{}.pickle\".format(outfile_time)\n",
    "    pickle.dump(sets_lists_selected, open(final_sets_name, \"wb\"))\n",
    "    \n",
    "    dataset = open(filename_data, 'r', encoding=\"utf-8\")\n",
    "    final_dataset_name = \"final_dataset_{}_{}.txt\".format(outfile_time, len(used_set))\n",
    "    final_dataset = open(final_dataset_name, \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    \n",
    "    line_nb = 0\n",
    "    data_h = {}\n",
    "    stop_new_file = False\n",
    "    while not stop_new_file:\n",
    "        line = dataset.readline()\n",
    "        line_nb += 1\n",
    "        if line_nb in used_set:\n",
    "            final_dataset.write(line)\n",
    "            data_h[line_nb] = line\n",
    "        if not line:\n",
    "            stop_new_file = True\n",
    "    final_dataset.close()\n",
    "    dataset.close()\n",
    "    \n",
    "    print(\"Extra files successfully created:\\n{}\\n{}\\n\".format(final_sets_name,final_dataset_name))\n",
    "\n",
    "    \n",
    "    ytrain_name = \"ytrain\"\n",
    "    yval_name = \"yval\"\n",
    "    ytest_name = \"ytest\"\n",
    "    \n",
    "    xtrain_name = \"xtrain\"\n",
    "    xval_name = \"xval\"\n",
    "    xtest_name = \"xtest\"\n",
    "    \n",
    "    to_dir = \"train_val_test_datasets-{}\".format(outfile_time)\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    dir_train = \"train_datasets\"\n",
    "    os.makedirs(os.path.join(to_dir,dir_train), exist_ok=True)\n",
    "    dir_val = \"val_datasets\"\n",
    "    os.makedirs(os.path.join(to_dir,dir_val), exist_ok=True)\n",
    "    dir_test = \"test_datasets\"\n",
    "    os.makedirs(os.path.join(to_dir,dir_test), exist_ok=True)\n",
    "    \n",
    "    # for each homophone of each pair of homophones (a,b), we create a training, validation and test set.\n",
    "    # for each homophone, the other homophone of the pair is used as a noise.\n",
    "    \n",
    "    # so a homophone h has 6 files (the pair is (h,h')): \n",
    "    # 2 files for trainig, one with h corectly written (ytrain), the other with h replaced by h' (xtrain) half of the time\n",
    "    # 2 validation files, one with h corectly written (yval), the other with h replaced by h' (xval) half of the time\n",
    "    # 2 files for testing, one with h corectly written (ytest), the other with h replaced by h' (xtest) half of the time\n",
    "    # files prefixed with x will be the inputs (effectively modeling a noisy input)\n",
    "    # files prefixed with y will be the targets (true labels)\n",
    "    # then we do the same but we reverse the role of h and h'\n",
    "    # we continue by doing the same for each pair of homophones\n",
    "    \n",
    "    \n",
    "    for (a,b) in homophones:\n",
    "        ytraina = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(ytrain_name,a,b,outfile_time)),\n",
    "                       \"w\", encoding=\"utf-8\")\n",
    "        yvala = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(yval_name,a,b,outfile_time)),\n",
    "                     \"w\", encoding=\"utf-8\")\n",
    "        ytesta = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}.txt\".format(ytest_name,a,b,outfile_time)),\n",
    "                      \"w\", encoding=\"utf-8\")\n",
    "        ytesta_incorrect = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}-incorrect.txt\".format(ytest_name,a,b,outfile_time)),\n",
    "                                \"w\", encoding=\"utf-8\")\n",
    "        xtraina = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(xtrain_name,a,b,outfile_time)),\n",
    "                       \"w\", encoding=\"utf-8\")\n",
    "        xvala = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(xval_name,a,b,outfile_time)),\n",
    "                     \"w\", encoding=\"utf-8\")\n",
    "        xtesta = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}.txt\".format(xtest_name,a,b,outfile_time)),\n",
    "                      \"w\", encoding=\"utf-8\")\n",
    "        xtesta_incorrect = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}-incorrect.txt\".format(xtest_name,a,b,outfile_time)),\n",
    "                                \"w\", encoding=\"utf-8\")\n",
    "        \n",
    "        counter_a = 0\n",
    "        for num in sets_lists_selected[a][b]:\n",
    "            counter_a += 1\n",
    "            ysample = \" \"+ data_h[num].strip() +\" \"\n",
    "            aspace = \" \" + a + \" \"\n",
    "            bspace = \" \" + b + \" \"\n",
    "            xsample = ysample.replace(aspace,bspace,1)\n",
    "            ysample = ysample.strip()+\"\\n\"\n",
    "            xsample = xsample.strip()+\"\\n\"\n",
    "            \n",
    "            if counter_a <= int(trainsize):\n",
    "                ytraina.write(ysample)\n",
    "                if counter_a % 2 == 0:\n",
    "                    xtraina.write(ysample)\n",
    "                else:\n",
    "                    xtraina.write(xsample)\n",
    "            if counter_a > int(trainsize) and counter_a <= int(trainsize*(1+valsize)):\n",
    "                yvala.write(ysample)\n",
    "                if counter_a % 2 == 0:\n",
    "                    xvala.write(ysample)\n",
    "                else:\n",
    "                    xvala.write(xsample)\n",
    "            if counter_a > int(trainsize*(1+valsize)):\n",
    "                if counter_a % 2 == 0:\n",
    "                    xtesta.write(ysample)\n",
    "                    ytesta.write(ysample)\n",
    "                else:\n",
    "                    xtesta_incorrect.write(xsample)\n",
    "                    ytesta_incorrect.write(ysample)\n",
    "            \n",
    "        ytraina.close()\n",
    "        yvala.close()\n",
    "        ytesta.close()\n",
    "        ytesta_incorrect.close()\n",
    "        xtraina.close()\n",
    "        xvala.close()\n",
    "        xtesta.close()\n",
    "        xtesta_incorrect.close()\n",
    "            \n",
    "        ytrainb = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(ytrain_name,b,a,outfile_time)),\n",
    "                       \"w\", encoding=\"utf-8\")\n",
    "        yvalb = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(yval_name,b,a,outfile_time)),\n",
    "                     \"w\", encoding=\"utf-8\")\n",
    "        ytestb = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}.txt\".format(ytest_name,b,a,outfile_time)),\n",
    "                      \"w\", encoding=\"utf-8\")\n",
    "        ytestb_incorrect = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}-incorrect.txt\".format(ytest_name,b,a,outfile_time)),\n",
    "                                \"w\", encoding=\"utf-8\")\n",
    "        xtrainb = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(xtrain_name,b,a,outfile_time)),\n",
    "                       \"w\", encoding=\"utf-8\")\n",
    "        xvalb = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(xval_name,b,a,outfile_time)),\n",
    "                     \"w\", encoding=\"utf-8\")\n",
    "        xtestb = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}.txt\".format(xtest_name,b,a,outfile_time)),\n",
    "                      \"w\", encoding=\"utf-8\")\n",
    "        xtestb_incorrect = open(os.path.join(to_dir,dir_test,\"{}-{}-{}-{}-incorrect.txt\".format(xtest_name,b,a,outfile_time)),\n",
    "                                \"w\", encoding=\"utf-8\")\n",
    "        \n",
    "        counter_b = 0\n",
    "        for num in sets_lists_selected[b][a]:\n",
    "            counter_b += 1\n",
    "            ysample = \" \"+ data_h[num].strip() +\" \"\n",
    "            bspace = \" \" + b + \" \"\n",
    "            aspace = \" \" + a + \" \"\n",
    "            xsample = ysample.replace(bspace,aspace,1)\n",
    "            ysample = ysample.strip()+\"\\n\"\n",
    "            xsample = xsample.strip()+\"\\n\"\n",
    "            \n",
    "            if counter_b <= int(trainsize):\n",
    "                ytrainb.write(ysample)\n",
    "                if counter_b % 2 == 0:\n",
    "                    xtrainb.write(ysample)\n",
    "                else:\n",
    "                    xtrainb.write(xsample)\n",
    "            if counter_b > int(trainsize) and counter_b <= int(trainsize*(1+valsize)):\n",
    "                yvalb.write(ysample)\n",
    "                if counter_b % 2 == 0:\n",
    "                    xvalb.write(ysample)\n",
    "                else:\n",
    "                    xvalb.write(xsample)\n",
    "            if counter_b > int(trainsize*(1+valsize)):\n",
    "                if counter_b % 2 == 0:\n",
    "                    xtestb.write(ysample)\n",
    "                    ytestb.write(ysample)\n",
    "                else:\n",
    "                    xtestb_incorrect.write(xsample)\n",
    "                    ytestb_incorrect.write(ysample)\n",
    "            \n",
    "        ytrainb.close()\n",
    "        yvalb.close()\n",
    "        ytestb.close()\n",
    "        ytestb_incorrect.close()\n",
    "        xtrainb.close()\n",
    "        xvalb.close()\n",
    "        xtestb.close()\n",
    "        xtestb_incorrect.close()\n",
    "        \n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"buil_train_val_test function executed successfully (~{} {})\\n\".format(d, d_unit))\n",
    "    print(\"Files are located in the following directory: {}\".format(to_dir))\n",
    "    print(\"{}\\n{}\\n{}\".format(dir_train,dir_val,dir_test))\n",
    "    print(\"Time index for the current split: {}\\n\".format(outfile_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_after_tokenization(tokenizer,data_filename,visualize=True,saveplot=False,ret=False,maxlen=60):\n",
    "    dataset = tf.data.TextLineDataset(data_filename)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    total_tokens=0\n",
    "    nb_lines = 0\n",
    "    maximum = 0\n",
    "    minimum = 60\n",
    "    dic_length = {i:0 for i in range(1,maxlen+1)}\n",
    "    for line in dataset:\n",
    "        nb_lines += 1\n",
    "        len_line = len(tokenizer.encode(line.numpy().decode(\"utf-8\")))\n",
    "        #if len_line >= 30 :\n",
    "        #    print(nb_lines)\n",
    "        #    print(line.numpy().decode(\"utf-8\"))\n",
    "        dic_length[len_line] += 1\n",
    "        total_tokens += len_line\n",
    "        if len_line > maximum:\n",
    "            maximum = len_line\n",
    "        #if nb_lines == 0:\n",
    "        #    minimum = len_line\n",
    "        if len_line < minimum:\n",
    "            minimum = len_line\n",
    "        \n",
    "    \n",
    "    mu = round(total_tokens/nb_lines,2)\n",
    "    #print(\"{} : {}\".format([\"s\",\"nb_lines\",\"s/nb_lines\",\"min\",\"max\"],[s,nb_lines, s/nb_lines, minimum, maximum]))\n",
    "    \n",
    "    if visualize:\n",
    "        plt.bar(list(dic_length.keys()),list(dic_length.values()))\n",
    "        plt.title(\"Tokenized sentence length (vocab_size={})\".format(vocab_size))\n",
    "        plt.xlabel(\"Sentence length after Tokenization\")\n",
    "        plt.ylabel(\"Number of sentenences\")\n",
    "        plt.text(0.75*maxlen,0.90*max(dic_length.values()),\"mean = {}\".format(mu))\n",
    "        plt.text(1.1*maxlen,0.90*max(dic_length.values()),\"Summary statistics:\")\n",
    "        plt.text(1.1*maxlen,0.83*max(dic_length.values()),\"Mean token number per sentence: {}\".format(mu))\n",
    "        plt.text(1.1*maxlen,0.76*max(dic_length.values()),\"Minimum sentence length: {}\".format(minimum))\n",
    "        plt.text(1.1*maxlen,0.69*max(dic_length.values()),\"Maximum sentence length: {}\".format(maximum))\n",
    "        plt.text(1.1*maxlen,0.62*max(dic_length.values()),\"Total number of tokens: {}\".format(total_tokens))\n",
    "        plt.text(1.1*maxlen,0.55*max(dic_length.values()),\"Total number of sentences: {}\".format(nb_lines))\n",
    "        #plt.xticks(range(0,maxlen+1,int(round((maxlen+1)/12))))\n",
    "        plt.show()\n",
    "    \n",
    "    if saveplot:\n",
    "        now = datetime.now()\n",
    "        plotdate = str(now.month)+\"-\"+str(now.day)+\"-\"+str(now.hour)+\"-\"+str(now.minute)\n",
    "        plotname = \"after_tokenization-{}-{}-{}.png\".format(vocab_size,data_filename,plotdate)\n",
    "        plt.savefig(plotname, dpi=600, bbox_inches = 'tight')\n",
    "        print(\"Plot successfully saved, \\n{}\".format(plotname))\n",
    "    \n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(\"Mean token number per sentence: {}\".format(mu))\n",
    "    print(\"Minimum sentence length: {} || Maximum sentence length: {}\".format(minimum,maximum))\n",
    "    print(\"Total number of tokens: {}\".format(total_tokens))\n",
    "    print(\"Total number of sentences: {}\".format(nb_lines))\n",
    "    \n",
    "    if ret:\n",
    "        return (dic_length,mu,minimum,maximum,total_tokens,nb_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_data = os.path.join(\"Clean_Data\",\"Selected_data-vs1024-toklim30.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_split function executed successfully (~0 seconds)\n",
      "\n",
      "Extra files successfully created:\n",
      "final_sets_selected_2-8-22-50.pickle\n",
      "final_dataset_2-8-22-50_140000.txt\n",
      "\n",
      "buil_train_val_test function executed successfully (~3 seconds)\n",
      "\n",
      "Files are located in the following directory: train_val_test_datasets-2-8-22-50\n",
      "train_datasets\n",
      "val_datasets\n",
      "test_datasets\n",
      "Time index for the current split: 2-8-22-50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buil_train_val_test(homophones, location_dict, filename_data, trainsize = 5000, valsize=1/5, testsize=1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_datasets_creation(homophones, to_dir, dir_train, dir_val, outfile_time, train_size, val_size, test_size=None):\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    ytrain_name = \"ytrain\"\n",
    "    yval_name = \"yval\"\n",
    "    #ytest_name = \"ytest\"\n",
    "    \n",
    "    xtrain_name = \"xtrain\"\n",
    "    xval_name = \"xval\"\n",
    "    #xtest_name = \"xtest\"\n",
    "    \n",
    "    \n",
    "    #os.path.join(to_dir, \"ytest-{}.txt\".format(outfile_time))\n",
    "    xtrain = open(\"xtrain-{}.txt\".format(outfile_time), \"w\", encoding=\"utf-8\")\n",
    "    ytrain = open(\"ytrain-{}.txt\".format(outfile_time), \"w\", encoding=\"utf-8\")\n",
    "    xval = open(\"xval-{}.txt\".format(outfile_time), \"w\", encoding=\"utf-8\")\n",
    "    yval = open(\"yval-{}.txt\".format(outfile_time), \"w\", encoding=\"utf-8\")\n",
    "    #xtest = open(os.path.join(to_dir, \"xtest-{}.txt\".format(outfile_time)), \"w\", encoding=\"utf-8\")\n",
    "    #ytest = open(os.path.join(to_dir, \"ytest-{}.txt\".format(outfile_time)), \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    for (a,b) in homophones:\n",
    "        xtraina = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(xtrain_name,a,b,outfile_time)),\n",
    "                       \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(xtrain_name,a,b,outfile_time))\n",
    "        ytraina = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(ytrain_name,a,b,outfile_time)),\n",
    "                       \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(ytrain_name,a,b,outfile_time))\n",
    "        xvala = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(xval_name,a,b,outfile_time)),\n",
    "                     \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(xval_name,a,b,outfile_time))\n",
    "        yvala = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(yval_name,a,b,outfile_time)),\n",
    "                     \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(yval_name,a,b,outfile_time))\n",
    "        #xtesta = open(\"{}-{}-{}-{}.txt\".format(xtest_name,a,b,outfile_time), \"r\", encoding=\"utf-8\")\n",
    "        #print(\"open file: {}-{}-{}-{}.txt\".format(xtest_name,a,b,outfile_time))\n",
    "        #ytesta = open(\"{}-{}-{}-{}.txt\".format(ytest_name,a,b,outfile_time), \"r\", encoding=\"utf-8\")\n",
    "        #print(\"open file: {}-{}-{}-{}.txt\".format(ytest_name,a,b,outfile_time))\n",
    "        \n",
    "        xtrainb = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(xtrain_name,b,a,outfile_time)),\n",
    "                       \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(xtrain_name,b,a,outfile_time))\n",
    "        ytrainb = open(os.path.join(to_dir,dir_train,\"{}-{}-{}-{}.txt\".format(ytrain_name,b,a,outfile_time)),\n",
    "                       \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(ytrain_name,b,a,outfile_time))\n",
    "        xvalb = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(xval_name,b,a,outfile_time)),\n",
    "                     \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(xval_name,b,a,outfile_time))\n",
    "        yvalb = open(os.path.join(to_dir,dir_val,\"{}-{}-{}-{}.txt\".format(yval_name,b,a,outfile_time)),\n",
    "                     \"r\", encoding=\"utf-8\")\n",
    "        print(\"open file: {}-{}-{}-{}.txt\".format(yval_name,b,a,outfile_time))\n",
    "        #xtestb = open(\"{}-{}-{}-{}.txt\".format(xtest_name,b,a,outfile_time), \"r\", encoding=\"utf-8\")\n",
    "        #print(\"open file: {}-{}-{}-{}.txt\".format(xtest_name,b,a,outfile_time))\n",
    "        #ytestb = open(\"{}-{}-{}-{}.txt\".format(ytest_name,b,a,outfile_time), \"r\", encoding=\"utf-8\")\n",
    "        #print(\"open file: {}-{}-{}-{}.txt\".format(ytest_name,b,a,outfile_time))\n",
    "        \n",
    "        counter_train = 0\n",
    "        while counter_train < train_size:\n",
    "            x_a = xtraina.readline()\n",
    "            x_b = xtrainb.readline()\n",
    "            y_a = ytraina.readline()\n",
    "            y_b = ytrainb.readline()\n",
    "            \n",
    "            xtrain.write(x_a)\n",
    "            xtrain.write(x_b)\n",
    "            ytrain.write(y_a)\n",
    "            ytrain.write(y_b)\n",
    "            \n",
    "            counter_train += 1 \n",
    "        \n",
    "        xtraina.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(xtrain_name,a,b,outfile_time))\n",
    "        xtrainb.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(xtrain_name,b,a,outfile_time))\n",
    "        ytraina.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(ytrain_name,a,b,outfile_time))\n",
    "        ytrainb.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(ytrain_name,b,a,outfile_time))\n",
    "        \n",
    "        counter_val = 0\n",
    "        while counter_val < val_size:\n",
    "            x_a = xvala.readline()\n",
    "            x_b = xvalb.readline()\n",
    "            y_a = yvala.readline()\n",
    "            y_b = yvalb.readline()\n",
    "            \n",
    "            xval.write(x_a)\n",
    "            xval.write(x_b)\n",
    "            yval.write(y_a)\n",
    "            yval.write(y_b)\n",
    "            \n",
    "            counter_val += 1\n",
    "         \n",
    "        xvala.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(xval_name,a,b,outfile_time))\n",
    "        xvalb.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(xval_name,b,a,outfile_time))\n",
    "        yvala.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(yval_name,a,b,outfile_time))\n",
    "        yvalb.close()\n",
    "        print(\"file closed: {}-{}-{}-{}.txt\".format(yval_name,b,a,outfile_time))\n",
    "        \n",
    "        #counter_test = 0\n",
    "        #while counter_test < test_size:\n",
    "        #    x_a = xtesta.readline()\n",
    "        #    x_b = xtestb.readline()\n",
    "        #    y_a = ytesta.readline()\n",
    "        #    y_b = ytestb.readline()\n",
    "            \n",
    "        #    xtest.write(x_a)\n",
    "        #    xtest.write(x_b)\n",
    "        #    ytest.write(y_a)\n",
    "        #    ytest.write(y_b)\n",
    "            \n",
    "        #    counter_test += 1\n",
    "        \n",
    "        #xtesta.close()\n",
    "        #print(\"file closed: {}-{}-{}-{}.txt\".format(xtest_name,a,b,outfile_time))\n",
    "        #xtestb.close()\n",
    "        #print(\"file closed: {}-{}-{}-{}.txt\".format(xtest_name,b,a,outfile_time))\n",
    "        #ytesta.close()\n",
    "        #print(\"file closed: {}-{}-{}-{}.txt\".format(ytest_name,a,b,outfile_time))\n",
    "        #ytestb.close()\n",
    "        #print(\"file closed: {}-{}-{}-{}.txt\".format(ytest_name,b,a,outfile_time))\n",
    "        \n",
    "    \n",
    "    print(\"\")\n",
    "    xtrain.close()\n",
    "    print(\"xtrain closed\")\n",
    "    ytrain.close()\n",
    "    print(\"ytrain closed\")\n",
    "    xval.close()\n",
    "    print(\"xval closed\")\n",
    "    yval.close()\n",
    "    print(\"yval closed\")\n",
    "    #xtest.close()\n",
    "    #print(\"xtest closed\")\n",
    "    #ytest.close()\n",
    "    #print(\"ytest closed\")\n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"\\nTraining set and Validation set successfully created (~{} {})\".format(d, d_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(\"Model_training\",\"checkpoints\",\"train\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_time = \"2-8-22-50\"\n",
    "to_dir = \"train_val_test_datasets-{}\".format(outfile_time)\n",
    "dir_train = \"train_datasets\"\n",
    "dir_val = \"val_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file: xtrain-a-à-2-8-22-50.txt\n",
      "open file: ytrain-a-à-2-8-22-50.txt\n",
      "open file: xval-a-à-2-8-22-50.txt\n",
      "open file: yval-a-à-2-8-22-50.txt\n",
      "open file: xtrain-à-a-2-8-22-50.txt\n",
      "open file: ytrain-à-a-2-8-22-50.txt\n",
      "open file: xval-à-a-2-8-22-50.txt\n",
      "open file: yval-à-a-2-8-22-50.txt\n",
      "file closed: xtrain-a-à-2-8-22-50.txt\n",
      "file closed: xtrain-à-a-2-8-22-50.txt\n",
      "file closed: ytrain-a-à-2-8-22-50.txt\n",
      "file closed: ytrain-à-a-2-8-22-50.txt\n",
      "file closed: xval-a-à-2-8-22-50.txt\n",
      "file closed: xval-à-a-2-8-22-50.txt\n",
      "file closed: yval-a-à-2-8-22-50.txt\n",
      "file closed: yval-à-a-2-8-22-50.txt\n",
      "open file: xtrain-est-et-2-8-22-50.txt\n",
      "open file: ytrain-est-et-2-8-22-50.txt\n",
      "open file: xval-est-et-2-8-22-50.txt\n",
      "open file: yval-est-et-2-8-22-50.txt\n",
      "open file: xtrain-et-est-2-8-22-50.txt\n",
      "open file: ytrain-et-est-2-8-22-50.txt\n",
      "open file: xval-et-est-2-8-22-50.txt\n",
      "open file: yval-et-est-2-8-22-50.txt\n",
      "file closed: xtrain-est-et-2-8-22-50.txt\n",
      "file closed: xtrain-et-est-2-8-22-50.txt\n",
      "file closed: ytrain-est-et-2-8-22-50.txt\n",
      "file closed: ytrain-et-est-2-8-22-50.txt\n",
      "file closed: xval-est-et-2-8-22-50.txt\n",
      "file closed: xval-et-est-2-8-22-50.txt\n",
      "file closed: yval-est-et-2-8-22-50.txt\n",
      "file closed: yval-et-est-2-8-22-50.txt\n",
      "open file: xtrain-ces-ses-2-8-22-50.txt\n",
      "open file: ytrain-ces-ses-2-8-22-50.txt\n",
      "open file: xval-ces-ses-2-8-22-50.txt\n",
      "open file: yval-ces-ses-2-8-22-50.txt\n",
      "open file: xtrain-ses-ces-2-8-22-50.txt\n",
      "open file: ytrain-ses-ces-2-8-22-50.txt\n",
      "open file: xval-ses-ces-2-8-22-50.txt\n",
      "open file: yval-ses-ces-2-8-22-50.txt\n",
      "file closed: xtrain-ces-ses-2-8-22-50.txt\n",
      "file closed: xtrain-ses-ces-2-8-22-50.txt\n",
      "file closed: ytrain-ces-ses-2-8-22-50.txt\n",
      "file closed: ytrain-ses-ces-2-8-22-50.txt\n",
      "file closed: xval-ces-ses-2-8-22-50.txt\n",
      "file closed: xval-ses-ces-2-8-22-50.txt\n",
      "file closed: yval-ces-ses-2-8-22-50.txt\n",
      "file closed: yval-ses-ces-2-8-22-50.txt\n",
      "open file: xtrain-ce-se-2-8-22-50.txt\n",
      "open file: ytrain-ce-se-2-8-22-50.txt\n",
      "open file: xval-ce-se-2-8-22-50.txt\n",
      "open file: yval-ce-se-2-8-22-50.txt\n",
      "open file: xtrain-se-ce-2-8-22-50.txt\n",
      "open file: ytrain-se-ce-2-8-22-50.txt\n",
      "open file: xval-se-ce-2-8-22-50.txt\n",
      "open file: yval-se-ce-2-8-22-50.txt\n",
      "file closed: xtrain-ce-se-2-8-22-50.txt\n",
      "file closed: xtrain-se-ce-2-8-22-50.txt\n",
      "file closed: ytrain-ce-se-2-8-22-50.txt\n",
      "file closed: ytrain-se-ce-2-8-22-50.txt\n",
      "file closed: xval-ce-se-2-8-22-50.txt\n",
      "file closed: xval-se-ce-2-8-22-50.txt\n",
      "file closed: yval-ce-se-2-8-22-50.txt\n",
      "file closed: yval-se-ce-2-8-22-50.txt\n",
      "open file: xtrain-ou-où-2-8-22-50.txt\n",
      "open file: ytrain-ou-où-2-8-22-50.txt\n",
      "open file: xval-ou-où-2-8-22-50.txt\n",
      "open file: yval-ou-où-2-8-22-50.txt\n",
      "open file: xtrain-où-ou-2-8-22-50.txt\n",
      "open file: ytrain-où-ou-2-8-22-50.txt\n",
      "open file: xval-où-ou-2-8-22-50.txt\n",
      "open file: yval-où-ou-2-8-22-50.txt\n",
      "file closed: xtrain-ou-où-2-8-22-50.txt\n",
      "file closed: xtrain-où-ou-2-8-22-50.txt\n",
      "file closed: ytrain-ou-où-2-8-22-50.txt\n",
      "file closed: ytrain-où-ou-2-8-22-50.txt\n",
      "file closed: xval-ou-où-2-8-22-50.txt\n",
      "file closed: xval-où-ou-2-8-22-50.txt\n",
      "file closed: yval-ou-où-2-8-22-50.txt\n",
      "file closed: yval-où-ou-2-8-22-50.txt\n",
      "open file: xtrain-la-là-2-8-22-50.txt\n",
      "open file: ytrain-la-là-2-8-22-50.txt\n",
      "open file: xval-la-là-2-8-22-50.txt\n",
      "open file: yval-la-là-2-8-22-50.txt\n",
      "open file: xtrain-là-la-2-8-22-50.txt\n",
      "open file: ytrain-là-la-2-8-22-50.txt\n",
      "open file: xval-là-la-2-8-22-50.txt\n",
      "open file: yval-là-la-2-8-22-50.txt\n",
      "file closed: xtrain-la-là-2-8-22-50.txt\n",
      "file closed: xtrain-là-la-2-8-22-50.txt\n",
      "file closed: ytrain-la-là-2-8-22-50.txt\n",
      "file closed: ytrain-là-la-2-8-22-50.txt\n",
      "file closed: xval-la-là-2-8-22-50.txt\n",
      "file closed: xval-là-la-2-8-22-50.txt\n",
      "file closed: yval-la-là-2-8-22-50.txt\n",
      "file closed: yval-là-la-2-8-22-50.txt\n",
      "open file: xtrain-tout-tous-2-8-22-50.txt\n",
      "open file: ytrain-tout-tous-2-8-22-50.txt\n",
      "open file: xval-tout-tous-2-8-22-50.txt\n",
      "open file: yval-tout-tous-2-8-22-50.txt\n",
      "open file: xtrain-tous-tout-2-8-22-50.txt\n",
      "open file: ytrain-tous-tout-2-8-22-50.txt\n",
      "open file: xval-tous-tout-2-8-22-50.txt\n",
      "open file: yval-tous-tout-2-8-22-50.txt\n",
      "file closed: xtrain-tout-tous-2-8-22-50.txt\n",
      "file closed: xtrain-tous-tout-2-8-22-50.txt\n",
      "file closed: ytrain-tout-tous-2-8-22-50.txt\n",
      "file closed: ytrain-tous-tout-2-8-22-50.txt\n",
      "file closed: xval-tout-tous-2-8-22-50.txt\n",
      "file closed: xval-tous-tout-2-8-22-50.txt\n",
      "file closed: yval-tout-tous-2-8-22-50.txt\n",
      "file closed: yval-tous-tout-2-8-22-50.txt\n",
      "open file: xtrain-leur-leurs-2-8-22-50.txt\n",
      "open file: ytrain-leur-leurs-2-8-22-50.txt\n",
      "open file: xval-leur-leurs-2-8-22-50.txt\n",
      "open file: yval-leur-leurs-2-8-22-50.txt\n",
      "open file: xtrain-leurs-leur-2-8-22-50.txt\n",
      "open file: ytrain-leurs-leur-2-8-22-50.txt\n",
      "open file: xval-leurs-leur-2-8-22-50.txt\n",
      "open file: yval-leurs-leur-2-8-22-50.txt\n",
      "file closed: xtrain-leur-leurs-2-8-22-50.txt\n",
      "file closed: xtrain-leurs-leur-2-8-22-50.txt\n",
      "file closed: ytrain-leur-leurs-2-8-22-50.txt\n",
      "file closed: ytrain-leurs-leur-2-8-22-50.txt\n",
      "file closed: xval-leur-leurs-2-8-22-50.txt\n",
      "file closed: xval-leurs-leur-2-8-22-50.txt\n",
      "file closed: yval-leur-leurs-2-8-22-50.txt\n",
      "file closed: yval-leurs-leur-2-8-22-50.txt\n",
      "open file: xtrain-ceux-ce-2-8-22-50.txt\n",
      "open file: ytrain-ceux-ce-2-8-22-50.txt\n",
      "open file: xval-ceux-ce-2-8-22-50.txt\n",
      "open file: yval-ceux-ce-2-8-22-50.txt\n",
      "open file: xtrain-ce-ceux-2-8-22-50.txt\n",
      "open file: ytrain-ce-ceux-2-8-22-50.txt\n",
      "open file: xval-ce-ceux-2-8-22-50.txt\n",
      "open file: yval-ce-ceux-2-8-22-50.txt\n",
      "file closed: xtrain-ceux-ce-2-8-22-50.txt\n",
      "file closed: xtrain-ce-ceux-2-8-22-50.txt\n",
      "file closed: ytrain-ceux-ce-2-8-22-50.txt\n",
      "file closed: ytrain-ce-ceux-2-8-22-50.txt\n",
      "file closed: xval-ceux-ce-2-8-22-50.txt\n",
      "file closed: xval-ce-ceux-2-8-22-50.txt\n",
      "file closed: yval-ceux-ce-2-8-22-50.txt\n",
      "file closed: yval-ce-ceux-2-8-22-50.txt\n",
      "open file: xtrain-cette-cet-2-8-22-50.txt\n",
      "open file: ytrain-cette-cet-2-8-22-50.txt\n",
      "open file: xval-cette-cet-2-8-22-50.txt\n",
      "open file: yval-cette-cet-2-8-22-50.txt\n",
      "open file: xtrain-cet-cette-2-8-22-50.txt\n",
      "open file: ytrain-cet-cette-2-8-22-50.txt\n",
      "open file: xval-cet-cette-2-8-22-50.txt\n",
      "open file: yval-cet-cette-2-8-22-50.txt\n",
      "file closed: xtrain-cette-cet-2-8-22-50.txt\n",
      "file closed: xtrain-cet-cette-2-8-22-50.txt\n",
      "file closed: ytrain-cette-cet-2-8-22-50.txt\n",
      "file closed: ytrain-cet-cette-2-8-22-50.txt\n",
      "file closed: xval-cette-cet-2-8-22-50.txt\n",
      "file closed: xval-cet-cette-2-8-22-50.txt\n",
      "file closed: yval-cette-cet-2-8-22-50.txt\n",
      "file closed: yval-cet-cette-2-8-22-50.txt\n",
      "\n",
      "xtrain closed\n",
      "ytrain closed\n",
      "xval closed\n",
      "yval closed\n",
      "\n",
      "Training set and Validation set successfully created (~0 seconds)\n"
     ]
    }
   ],
   "source": [
    "final_datasets_creation(homophones,to_dir,dir_train,dir_val,outfile_time, 5000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move the newly created files (xtrain, ytrain, xval, yval) in .\\Model_training\n",
    "### Move also the tokenizer 'tokenizer_1024_file_train_tokenizer-133000lines.txt' in .\\Model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_two_files(dr1,name1, dr2, name2, to_dr, nname):\n",
    "    inp1 = open(os.path.join(dr1,name1), \"r\", encoding=\"utf-8\")\n",
    "    inp2 = open(os.path.join(dr2,name2), \"r\", encoding=\"utf-8\")\n",
    "    out = open(os.path.join(to_dr,nname), \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    stop1 = False\n",
    "    counter1 = 0\n",
    "    print(\"start processing {}\".format(name1))\n",
    "    while not stop1:\n",
    "        line = inp1.readline()\n",
    "        if line:\n",
    "            out.write(line)\n",
    "            counter1 += 1\n",
    "        if not line:\n",
    "            stop1 = True\n",
    "            print(\"finished processing {} ({} lines added)\".format(name1, counter1))\n",
    "    \n",
    "    stop2 = False\n",
    "    counter2 = 0\n",
    "    print(\"start processing {}\".format(name2))\n",
    "    while not stop2:\n",
    "        line = inp2.readline()\n",
    "        out.write(line)\n",
    "        counter2 += 1\n",
    "        if not line:\n",
    "            stop2 = True\n",
    "            print(\"finished processing {} ({} lines added)\".format(name2, counter2))\n",
    "    \n",
    "    inp1.close()\n",
    "    inp2.close()\n",
    "    out.close()\n",
    "    \n",
    "    end = datetime.now()\n",
    "    delta = end - start\n",
    "    delta_minutes = delta.seconds // 60\n",
    "    d = delta_minutes\n",
    "    d_unit = \"minutes\"\n",
    "    if delta_minutes == 0:\n",
    "        d = delta.seconds\n",
    "        d_unit = \"seconds\"\n",
    "    print(\"File created with success: ({} lines in new file, ~{} {})\\n{}\\n\".format(counter1+counter2, d, d_unit, nname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-2.1.0",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
